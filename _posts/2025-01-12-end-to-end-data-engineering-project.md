---
title: "End-to-End Data Engineering Project ‚Äì Amazon Delivery"
date: 2025-01-12
categories: [Data Engineering, Cloud, Snowflake, AWS]
tags: [Data Engineering, Snowflake, AWS, S3, SQL, Python]
---

## üöö End-to-End Data Engineering Project ‚Äì Amazon Delivery (Last Mile)
![Amazon End-to-End](/assets/images/Amazon_Problema.png)


En este proyecto dise√±√© e implement√© una **arquitectura end-to-end de ingenier√≠a de datos**, simulando un escenario real de **log√≠stica y entregas de √∫ltima milla**.  
El objetivo fue transformar datos crudos en informaci√≥n confiable y lista para an√°lisis, aplicando **buenas pr√°cticas de Data Engineering modernas**.
---

## üéØ Problema de negocio

La mayoria de empresas tienen los siguientes problemas al momento de obtener los datos de ultima milla:

- Archivos de diferentes fuentes SQL, SCV, EXCEL,Bases de datos
- Tener fechas, horas y coordenadas mal tipadas
- Mezclar datos hist√≥ricos con datos recientes
- No estar listos para an√°lisis ni KPIs operativos
- Una gran base de datos pero no toda es necesaria
- Estructura de que queremos extraer de esa gran cantidad de datos

Esto dificulta responder preguntas clave como:

- ¬øCu√°nto tarda una entrega?
- ¬øQu√© repartidores son m√°s eficientes?
- ¬øQu√© zonas presentan m√°s demoras o devoluciones?
- ¬øcada cuanto necesito que mis datos se actualicen?
- ¬øEs realmente efectiva mi logistica de ultima milla respondiendo a estos KPIS?

---

## Teniendo encuenta estos problemas planteo la siguiente Solucion Dise√±ar 
## Una Arquitectura Enterprice Desacoplada y escalable basada en:

- **Amazon S3** como Data Lake 
- **Snowflake** como Data Warehouse 
- Separaci√≥n clara por capas:
  - **RAW** ‚Üí datos crudos
  - **STAGING** ‚Üí limpieza y tipado
  - **MART** ‚Üí consumo anal√≠tico
Esto es algo Sencillo pero cubre esta necesidad!!!
---


## üèóÔ∏è Arquitectura de Datos

![Arquitectura End-to-End Amazon Delivery](/assets/images/arquitectura_end_to_end_v1.png)


**Flujo general:**

1. Generaci√≥n y expansi√≥n de datos con **Python**
2. Almacenamiento en **Amazon S3**
3. Ingesta en **Snowflake (RAW)**
4. Limpieza y transformaci√≥n en **STAGING**
5. Preparaci√≥n para anal√≠tica en **MART**
6. Reglas de negocio con DBT **VISUAL STUDIO CODE**

---

## üß™ Generaci√≥n y preparaci√≥n de datos (Python)

- Dataset base de entregas
- Expansi√≥n temporal de **2022 a 2025**
- Simulaci√≥n de crecimiento del negocio
- Normalizaci√≥n de columnas
- Exportaci√≥n final para ingesta

Esto permite simular un entorno real con **volumen creciente y datos hist√≥ricos**.
Solo tenia datos del 2022 lo que realice fue simular los demas a√±os, 
para poder practicar reglas de negocio para practicar DBT

---

## ‚òÅÔ∏è Data Lake ‚Äì Amazon S3

![Amazon S3 Bucket](../assets/img/s3_bucket.png)

- Bucket: `amazon-deliveryml`
- Almacenamiento de archivos CSV
- Fuente √∫nica de verdad
- Arquitectura desacoplada (storage ‚â† compute)

---

## ‚ùÑÔ∏è Data Warehouse ‚Äì Snowflake (RAW)

![Snowflake RAW](../assets/img/snowflake_raw.png)

En la capa **RAW**:

- Se crea un **External Stage** apuntando a S3
- Se define un **File Format CSV**
- Se cargan los datos usando `COPY INTO`
- Los datos se almacenan **sin transformaciones**

Esta capa preserva la informaci√≥n original tal como llega.

---

## üßº Capa STAGING ‚Äì Limpieza y tipado

![Snowflake STAGING](../assets/img/snowflake_staging.png)

En **STAGING** se aplican transformaciones clave:

- Conversi√≥n segura de tipos:
  - `TRY_TO_DATE`
  - `TRY_TO_TIMESTAMP`
  - `TRY_TO_DOUBLE`
- Manejo de valores inconsistentes
- C√°lculo de m√©tricas operativas:
  - ‚è±Ô∏è `DELIVERY_TIME_MIN`
- Normalizaci√≥n de coordenadas geogr√°ficas
- Enriquecimiento temporal (YEAR, LOAD_TS)

Aqu√≠ los datos quedan **confiables y listos para an√°lisis**.

---

## üìä Capa MART ‚Äì Consumo anal√≠tico

![Snowflake MART](../assets/img/snowflake_raw.png)

La capa **MART** queda preparada para:

- Dashboards en Power BI / Tableau
- KPIs operativos
- An√°lisis de productividad
- Modelos de Machine Learning
- Feature engineering

Ejemplos de uso:
- Tiempo promedio de entrega
- Rendimiento por repartidor
- Zonas con m√°s devoluciones
- Impacto del tr√°fico y clima

---

## üìä DBT  ‚Äì Reglas de Negocio

En esta etapa se implementaron reglas de negocio y modelos anal√≠ticos usando dbt,
con el objetivo de transformar los datos limpios en m√©tricas 
confiables y reutilizables para el negocio.

Principales tareas realizadas:

- Definici√≥n de modelos anal√≠ticos a partir de la capa STAGING
- C√°lculo de KPIs operativos (tiempos de entrega, rendimiento por repartidor)
- Estandarizaci√≥n de m√©tricas para consumo en BI y anal√≠tica avanzada
- Aplicaci√≥n de buenas pr√°cticas de transformaciones versionadas y reproducibles

El uso de dbt permiti√≥ separar claramente la l√≥gica de negocio del procesamiento t√©cnico, 
facilitando la escalabilidad, el mantenimiento y la trazabilidad de los datos.


## üß© Tecnolog√≠as utilizadas

- **Python** (Pandas, NumPy)
- **Amazon S3**
- **Snowflake**
  - Stages
  - File Formats
  - COPY INTO
  - SQL transformations
- **Arquitectura Medall√≥n**
- SQL anal√≠tico

---

## üß† Conclusi√≥n

Que Realmente Aprendi de esta implementacion:

- Entender La necesidad de Negocio y que herramientas poder Utilizar
- Me cuestione en algun momento si esto necesariamente deberia utilizar arquitectura Enterprice
- Por que utilice estas Herramientas y no otras
- Convertir datos crudos en informaci√≥n accionable
- Aplicar buenas pr√°cticas modernas de Data Engineering

## üëΩ Reflexion:
Al avanzar en la implementaci√≥n de la soluci√≥n, confirm√© que la ingenier√≠a de datos no empieza con la tecnolog√≠a,
sino con el entendimiento del negocio. Antes de escribir una sola l√≠nea de c√≥digo, es clave comprender qu√© preguntas se quieren responder,
con qu√© nivel de precisi√≥n y en qu√© momento.
Otro aprendizaje importante fue dimensionar de forma realista el volumen de datos que se mover√° a lo largo del pipeline, 
ya que esto impacta directamente en costos, rendimiento y decisiones de arquitectura. No todo necesita ser ‚Äúbig data‚Äù, 
pero todo debe estar bien dise√±ado.
Este ejercicio tambi√©n me permiti√≥ entender que una buena soluci√≥n no es la m√°s compleja, 
sino la que resuelve el problema correcto de forma sostenible, dejando el camino preparado para crecer.
Dise√±ar capas claras (RAW, STAGING, MART) y separar la l√≥gica t√©cnica de la l√≥gica de negocio facilita el mantenimiento, la escalabilidad y la adopci√≥n por otros equipos.

Finalmente, entend√≠ que los proyectos de datos son iterativos. 
Con mayor madurez t√©cnica y mayor contexto del negocio, volver a evaluar decisiones pasadas es parte natural del proceso. 
Este proyecto no es un punto final, sino una base s√≥lida sobre la cual seguir optimizando, mejorando y profesionalizando la soluci√≥n.

---

## üöÄ Pr√≥ximos pasos

Este proyecto queda listo para extenderse con:

- Orquestaci√≥n (Airflow / Prefect)
- Cargas incrementales
- Dashboards BI
- Feature Store para ML
- Automatizaci√≥n end-to-end


üìå **Repositorio y m√°s proyectos en mi perfil.**

